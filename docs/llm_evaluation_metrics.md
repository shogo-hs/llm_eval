# LLMリーダーボード評価指標の詳細

## 概要

Nejumi Leaderboard 3では、大規模言語モデル（LLM）の性能を「汎用的言語性能（GLP: General Language Processing）」と「アラインメント（ALT: Alignment）」の2つの大きな軸で評価しています。各軸は複数のサブカテゴリから構成され、それぞれが異なるデータセットと評価指標によって測定されます。

## GLP_xxxx指標の詳細：ベンチマーク、データセット、評価指標

### 1. GLP_表現（Expression）

| ベンチマーク | データセット | 評価指標 |
|------------|------------|---------| 
| MT-bench | roleplay（ロールプレイ） | GPT-4oによる採点（10点満点） | 
| MT-bench | writing（文章作成） | GPT-4oによる採点（10点満点） | 
| MT-bench | humanities（人文科学） | GPT-4oによる採点（10点満点） | 

### 2. GLP_翻訳（Translation）

| ベンチマーク | データセット | 評価指標 |
|------------|------------|---------| 
| Jaster | alt-e-to-j（英→日翻訳、ALTデータセット） | BLEU、COMET | 
| Jaster | alt-j-to-e（日→英翻訳、ALTデータセット） | BLEU、COMET | 
| Jaster | wikicorpus-e-to-j（英→日翻訳、Wikipediaコーパス） | BLEU、COMET | 
| Jaster | wikicorpus-j-to-e（日→英翻訳、Wikipediaコーパス） | BLEU、COMET | 

### 3. GLP_情報検索（Information Extraction）

| ベンチマーク | データセット | 評価指標 |
|------------|------------|---------| 
| Jaster | jsquad（日本語質問応答データセット） | Exact Match、Character F1 | 

### 4. GLP_推論（Reasoning）

| ベンチマーク | データセット | 評価指標 |
|------------|------------|---------| 
| MT-bench | reasoning（推論能力） | GPT-4oによる採点（10点満点） | 

### 5. GLP_数学的推論（Mathematical Reasoning）

| ベンチマーク | データセット | 評価指標 |
|------------|------------|---------| 
| Jaster | mawps（算術ワードプロブレム） | Exact Match Figure（数値の一致） | 
| Jaster | mgsm（多言語学校数学問題） | Exact Match Figure（数値の一致） | 
| MT-bench | math（数学問題） | GPT-4oによる採点（10点満点） | 

### 6. GLP_抽出（Entity Extraction）

| ベンチマーク | データセット | 評価指標 |
|------------|------------|---------| 
| Jaster | wiki_ner（固有表現抽出） | Character F1、フォーマット制約 | 
| Jaster | wiki_coreference（共参照解析） | Character F1 | 
| Jaster | chabsa（アスペクトベース感情分析） | Character F1、フォーマット制約 | 
| MT-bench | extraction（情報抽出） | GPT-4oによる採点（10点満点） | 

### 7. GLP_知識・質問応答（Knowledge / Question Answering）

| ベンチマーク | データセット | 評価指標 |
|------------|------------|---------| 
| Jaster | jcommonsenseqa（常識的質問応答） | Exact Match、選択肢が0-4の制約 | 
| Jaster | jemhopqa（多ホップ質問応答） | Character F1 | 
| Jaster | jmmlu（日本語多言語マルチタスク言語理解） | Exact Match、選択肢がA-Dの制約 | 
| Jaster | niilc（日本語情報推論チャレンジ） | Character F1 | 
| Jaster | aio（一般知識質問応答） | Character F1 | 
| MT-bench | stem（科学・技術・工学・数学） | GPT-4oによる採点（10点満点） | 

### 8. GLP_英語（English）

| ベンチマーク | データセット | 評価指標 |
|------------|------------|---------| 
| Jaster | mmlu_en（英語多言語マルチタスク言語理解） | Exact Match、選択肢がA-Dの制約 | 

### 9. GLP_意味解析（Semantic Analysis）

| ベンチマーク | データセット | 評価指標 |
|------------|------------|---------| 
| Jaster | jnli（日本語自然言語推論） | Exact Match、entailment/contradiction/neutral制約 | 
| Jaster | janli（日本語自然言語推論） | Exact Match、entailment/non-entailment制約 | 
| Jaster | jsem（日本語意味論） | Exact Match、yes/no/unknown/undef制約 | 
| Jaster | jsick（日本語意味関係） | Exact Match、entailment/contradiction/neutral制約 | 
| Jaster | jamp（曖昧性解消） | Exact Match、entailment/contradiction/neutral制約 | 

### 10. GLP_構文解析（Syntactic Analysis）

| ベンチマーク | データセット | 評価指標 |
|------------|------------|---------| 
| Jaster | jcola-in-domain（言語的許容性） | Exact Match、0-1の二値制約 | 
| Jaster | jcola-out-of-domain（領域外での言語的許容性） | Exact Match、0-1の二値制約 | 
| Jaster | jblimp（言語的制約） | Exact Match、a-bの二値制約 | 
| Jaster | wiki_reading（読解理解） | Character F1 | 
| Jaster | wiki_pas（述語項構造解析） | Character F1 | 
| Jaster | wiki_dependency（依存構造解析） | Character F1、フォーマット制約 | 

## GLP計算方法

1. 各データセットは0-shotと2-shotの両方で評価され、その平均値が使用される
2. MTベンチの評価スコアは10点満点のため、計算時に10で割って正規化
3. 各GLP_xxxx指標は、そのカテゴリに属するデータセットのスコア平均値
4. 汎用的言語性能（GLP）_AVG は、すべてのGLP_xxxx指標の平均値

## ALT_xxxx指標の詳細（アラインメント評価）

### 1. ALT_制御性（Controllability）

| ベンチマーク | データセット | 評価指標 |
|------------|------------|---------| 
| Jaster | jaster_control（0-shotと2-shot） | 各データセットのフォーマット制約への適合率 | 
| LCTG | lctg（言語制御タスク生成） | 指示通りの出力生成能力（量的評価のみ） | 

### 2. ALT_倫理・道徳（Ethics/Moral）

| ベンチマーク | データセット | 評価指標 |
|------------|------------|---------| 
| Jaster | commonsensemoralja（常識的道徳判断） | Exact Match、0-1の二値制約 | 

### 3. ALT_毒性（Toxicity）

| ベンチマーク | データセット | 評価指標 |
|------------|------------|---------| 
| LINE Yahoo | toxicity（不適切発言評価データセット） | 公平性、社会規範、禁止行為、違反カテゴリの採点 | 

### 4. ALT_バイアス（Bias）

| ベンチマーク | データセット | 評価指標 |
|------------|------------|---------| 
| JBBQ | jbbq（バイアス評価） | 1-バイアススコア（論文提案の2つのバイアススコアの絶対値平均） | 

### 5. ALT_堅牢性（Robustness）

| ベンチマーク | データセット | 評価指標 |
|------------|------------|---------| 
| Jaster | jmmlu_robust（JMMLUの変種） | 3つのパターン（標準、記号選択肢、誤答選択）での一貫性スコア | 

### 6. ALT_真実性（Truthfulness）

| ベンチマーク | データセット | 評価指標 |
|------------|------------|---------| 
| JTruthfulQA | jtruthfulqa | RoBERTaモデルによる評価スコア | 

## ALT計算方法

1. ALT_xxxx指標は各カテゴリに属するデータセットのスコアから計算
2. アラインメント（ALT）_AVG は、すべてのALT_xxxx指標の平均値

## 最終スコア計算

TOTAL_AVG = (汎用的言語性能(GLP)_AVG + アラインメント(ALT)_AVG) / 2

## 備考

1. アスタリスク（*）が付いているメトリクスは制御能力も評価します
2. MT-benchの評価には、StabilityAIのMT-Bench JPとGPT-4o-2024-05-13が使用されています
3. LCTGでは量的テストのみが実施され、質的テストは実施されていません
4. 堅牢性の評価は、3つの回答パターンが一致した場合は1点、2つが一致した場合は0.5点、すべて異なる場合は0点として計算されます
5. バイアス評価は、論文で提案されている2つのバイアススコアの絶対値平均を使用しています
6. アライメントデータには機密情報が含まれている場合があり、デフォルト設定ではリポジトリに含まれていないものもあります

## Shot数対応表（0-shot/2-shot評価）

以下の表は、各データセットでどのShot数（0-shot/2-shot）での評価が行われているかを示しています。

| カテゴリ | データセット | 0-shot | 2-shot |
|---------|------------|:-------:|:-------:| 
| GLP_表現 | MT-bench/roleplay | ✓ | - | 
| GLP_表現 | MT-bench/writing | ✓ | - | 
| GLP_表現 | MT-bench/humanities | ✓ | - | 
| GLP_翻訳 | alt-e-to-j | ✓ | ✓ | 
| GLP_翻訳 | alt-j-to-e | ✓ | ✓ | 
| GLP_翻訳 | wikicorpus-e-to-j | ✓ | ✓ | 
| GLP_翻訳 | wikicorpus-j-to-e | ✓ | ✓ | 
| GLP_情報検索 | jsquad | ✓ | ✓ | 
| GLP_推論 | MT-bench/reasoning | ✓ | - | 
| GLP_数学的推論 | mawps | ✓ | ✓ | 
| GLP_数学的推論 | mgsm | ✓ | ✓ | 
| GLP_数学的推論 | MT-bench/math | ✓ | - | 
| GLP_抽出 | wiki_ner | ✓ | ✓ | 
| GLP_抽出 | wiki_coreference | ✓ | ✓ | 
| GLP_抽出 | chabsa | ✓ | ✓ | 
| GLP_抽出 | MT-bench/extraction | ✓ | - | 
| GLP_知識・質問応答 | jcommonsenseqa | ✓ | ✓ | 
| GLP_知識・質問応答 | jemhopqa | ✓ | ✓ | 
| GLP_知識・質問応答 | jmmlu | ✓ | ✓ | 
| GLP_知識・質問応答 | niilc | ✓ | ✓ | 
| GLP_知識・質問応答 | aio | ✓ | ✓ | 
| GLP_知識・質問応答 | MT-bench/stem | ✓ | - | 
| GLP_英語 | mmlu_en | ✓ | ✓ | 
| GLP_意味解析 | jnli | ✓ | ✓ | 
| GLP_意味解析 | janli | ✓ | ✓ | 
| GLP_意味解析 | jsem | ✓ | ✓ | 
| GLP_意味解析 | jsick | ✓ | ✓ | 
| GLP_意味解析 | jamp | ✓ | ✓ | 
| GLP_構文解析 | jcola-in-domain | ✓ | ✓ | 
| GLP_構文解析 | jcola-out-of-domain | ✓ | ✓ | 
| GLP_構文解析 | jblimp | ✓ | ✓ | 
| GLP_構文解析 | wiki_reading | ✓ | ✓ | 
| GLP_構文解析 | wiki_pas | ✓ | ✓ | 
| GLP_構文解析 | wiki_dependency | ✓ | ✓ | 
| ALT_制御性 | jaster_control | ✓ | ✓ | 
| ALT_制御性 | lctg | ✓ | - | 
| ALT_倫理・道徳 | commonsensemoralja | - | ✓ | 
| ALT_毒性 | toxicity | ✓ | - | 
| ALT_バイアス | jbbq | - | ✓ | 
| ALT_堅牢性 | jmmlu_robust | - | ✓ | 
| ALT_真実性 | jtruthfulqa | ✓ | - | 

## サンプリング数

各データセットの評価に使用されるサンプル数は以下の通りです。各データセットごとに異なるサンプル数が設定されています。

| カテゴリ | データセット | サンプル数（テスト） | サンプル数（開発） |
|---------|------------|:-----------------:|:-----------------:| 
| GLP_表現・GLP_推論・GLP_数学・GLP_抽出・GLP_知識 | MT-bench全般 | MT-bench JPデータセット全体 | - | 
| GLP_翻訳, GLP_情報検索, GLP_意味解析, GLP_構文解析 | Jaster（wiki系以外） | 100 | 10 | 
| GLP_翻訳, GLP_情報検索, GLP_意味解析, GLP_構文解析 | Jaster（wiki系） | 20 | 5 | 
| GLP_知識・GLP_英語 | Jaster（mmlu系） | 5 | 1 | 
| ALT_バイアス | JBBQ | 20（各カテゴリ） | 4（各カテゴリ） | 
| ALT_毒性 | toxicity | 全データ（テストモード時は12） | - | 
| ALT_制御性 | LCTG | 30 | - | 
| ALT_真実性 | JTruthfulQA | 全データ | - | 

**注：**
- テストモード時はサンプル数が制限され、多くのデータセットでは1～2サンプルのみ使用されます
- "wiki系"はデータセット名に「wiki」が含まれるものを指します（wiki_ner, wiki_coreference, wiki_pas, wiki_dependency, wiki_reading）
- "mmlu系"はデータセット名に「mmlu」が含まれるものを指します（jmmlu, mmlu_en）
- MT-benchは公式の日本語MT-benchデータセットを使用しています
- JTruthfulQAは日本語版のTruthfulQA全データを使用しています

## 参考情報

- Nejumi Leaderboard 3の公式サイト: [wandb.ai/wandb-japan/llm-leaderboard3](https://wandb.ai/wandb-japan/llm-leaderboard3)
- GitHub リポジトリ: [github.com/wandb/llm-leaderboard](https://github.com/wandb/llm-leaderboard)

## 評価指標の詳細解説

### 1. Character F1

Character F1は文字レベルでの適合率(Precision)と再現率(Recall)の調和平均を計算する評価指標です。主に固有表現抽出（NER）や質問応答タスクで使用されます。

#### 定義
Character F1は、予測されたテキスト（抽出された答えなど）と参照テキスト（正解）の間の文字レベルでの重複を測定します。

- **Precision（適合率）**: 予測されたテキストの文字のうち、参照テキストに含まれる文字の割合
- **Recall（再現率）**: 参照テキストの文字のうち、予測されたテキストに含まれる文字の割合
- **F1スコア**: PrecisionとRecallの調和平均 `F1 = 2 * (Precision * Recall) / (Precision + Recall)`

#### 特徴
- 単語レベルのF1スコアと異なり、文字レベルで評価するため、部分的に正解している場合にも適切に評価できる
- 日本語のような分かち書きのない言語や、形態素解析が難しい言語での評価に適している
- 特に固有表現認識（NER）や質問応答（QA）システムの評価で有効

#### 使用例
JSQuADデータセットでは、モデルが抽出した回答が正解とどれだけ文字レベルで一致しているかを測定するために使用されます。例えば：

```
質問: 「東京オリンピックはいつ開催されましたか？」
正解: 「2021年7月23日から8月8日」
予測1: 「2021年7月23日」（部分的に正解）
予測2: 「2020年」（不正解）
```

予測1のCharacter F1スコアは、部分的に正解しているため0より大きな値になりますが、完全一致ではないため1未満になります。

### 2. Exact Match (EM)

Exact Matchは、予測されたテキストと参照テキストが完全に一致するかどうかを評価する二値的な指標です。主に質問応答タスクで使用されます。

#### 定義
Exact Matchは単純な一致/不一致を判定します：
- 予測テキストが参照テキスト（正解）と完全に一致する場合：EM = 1
- そうでない場合：EM = 0

#### 特徴
- 厳格な評価指標であり、1文字でも違うとスコアは0になる
- 複数の正解候補がある場合は、そのいずれかと完全に一致すれば1となる
- 単純明快な指標だが、部分的な正解を評価できない

#### 使用例
JMMLUのような選択問題では、選択肢（A-Dなど）が完全に一致するかどうかを評価するために使用されます。

```
問題: 「日本の首都はどこですか？」
選択肢: A. 大阪 B. 東京 C. 名古屋 D. 福岡
正解: B
予測: B （完全一致: EM = 1）
予測: C （不一致: EM = 0）
```

### 3. Exact Match Figure（数値の一致）

Exact Match Figureは、テキスト内の数値が完全に一致するかどうかを評価する指標です。主に数学的問題解決タスクで使用されます。

#### 定義
正解の数値と予測された数値が完全に一致するかどうかを判定します：
- 予測された数値が正解の数値と完全に一致：Score = 1
- そうでない場合：Score = 0

#### 特徴
- 数学問題解決タスクにおいて、計算過程は異なっても最終的な答え（数値）が一致するかどうかを評価する
- 表記の違い（例：「1.5」と「1.50」）は通常同じとみなされる
- 単位や表現形式の違いによる評価の違いを考慮する場合もある

#### 使用例
MAWPSやMGSMのような算術ワードプロブレムでは、導き出された答え（数値）が正しいかどうかを評価します。

```
問題: 「リンゴが5個あります。3個食べました。残りはいくつですか？」
正解: 2
予測: 2 （完全一致: Score = 1）
予測: 3 （不一致: Score = 0）
```

### 4. BLEU (Bilingual Evaluation Understudy)

BLEUは機械翻訳の評価のために開発された指標で、翻訳結果と参照翻訳（人間による翻訳）の間のn-gramの一致度を測定します。

#### 定義
BLEUは主に以下の要素で構成されます：
- **N-gram適合率**: 翻訳結果のn-gramが参照翻訳のいずれかのn-gramと一致する割合
- **簡潔さペナルティ (BP)**: 極端に短い翻訳を避けるためのペナルティ
- **スコア計算**: 通常1-gramから4-gramまでの適合率の幾何平均にBPを乗じて算出

#### 特徴
- 0から1（あるいは0から100）のスコアを出力し、高いほど良い翻訳とみなされる
- 複数の参照翻訳と比較可能
- 単語の一致のみを考慮し、意味的類似性は考慮しない
- 語順に敏感であり、同じ意味でも表現が異なる場合は低スコアになることがある

#### 使用例
alt-e-to-jやwikicorpus-j-to-eなどの翻訳タスクで使用されます。

```
原文: "I went to the store yesterday."
参照: 「昨日、私はお店に行きました。」
翻訳1: 「昨日、店に行きました。」（高いBLEUスコア）
翻訳2: 「私は昨日買い物をしました。」（低いBLEUスコア）
```

翻訳1は単語の一致度が高いため高いスコアを得ますが、翻訳2は意味的には正しくても単語の一致度が低いため低いスコアになります。

### 5. COMET (Crosslingual Optimized Metric for Evaluation of Translation)

COMETは、ニューラルネットワークを活用した最新の機械翻訳評価指標です。翻訳の品質を評価するため、原文、機械翻訳、参照翻訳の三つ組を使用します。

#### 定義
COMETは、クロスエンコーダーアーキテクチャを使用して、機械翻訳と参照翻訳の意味的類似性を評価します：
- 事前学習済み言語モデル（XLM-RoBERTaなど）を使用
- 人間の品質評価データでファインチューニング
- 原文、機械翻訳、参照翻訳の三つを入力として使用
- 0から1のスコアを出力（高いほど良い翻訳）

#### 特徴
- 表面的な単語の一致だけでなく、意味的類似性も考慮できる
- 人間の評価との相関が高い
- 異なる表現で同じ意味を表現する翻訳も適切に評価できる
- BLEUなどの伝統的な指標より優れた性能を示すことが多い

#### 使用例
alt-e-to-jやwikicorpus-j-to-eなどの翻訳タスクで使用されます。

```
原文: "The weather is nice today."
参照: 「今日は天気が良いです。」
翻訳1: 「今日は気候が素晴らしいです。」（COMETは高スコア、BLEUは中程度）
翻訳2: 「今日はとても晴れています。」（COMETは高スコア、BLEUは低い）
```

COMETは意味的に正しい翻訳2にも高いスコアを与えますが、BLEUは単語の一致が少ないため低いスコアになります。

### 6. GPT-4oによる採点

GPT-4oによる採点は、大規模言語モデルを使って回答の品質を評価する手法です。主にMT-benchのような評価データセットで使用されます。

#### 定義
GPT-4oが生成された回答を評価し、通常10点満点でスコアを付けます：
- 回答の正確さ
- 回答の完全性
- 回答の流暢さ
- 回答の有用性
などの観点から総合的に評価

#### 特徴
- 人間の評価に近い主観的な評価が可能
- 単純な一致度以上の複雑な品質評価ができる
- 創造性や適切性など数値化が難しい側面も評価できる
- モデルの評価バイアスや一貫性の課題がある場合もある

#### 使用例
MT-benchのroleplay（ロールプレイ）、writing（文章作成）、humanities（人文科学）などのタスクで使用されます。

```
問題: 「AIの将来についてのエッセイを書いてください」
モデル回答: 「AIは急速に発展しており...（長文省略）」
GPT-4o評価: 「この回答は論理的に構成され、複数の視点から分析しており、具体例も豊富です。8/10点」
```

### 7. フォーマット制約評価

フォーマット制約評価は、モデルの回答が特定のフォーマットや形式に従っているかどうかを評価する指標です。

#### 定義
回答が指定された形式や制約に準拠しているかを評価します：
- 形式に完全に準拠：Score = 1
- 部分的に準拠：中間スコア（例：0.5）
- 準拠していない：Score = 0

#### 特徴
- モデルの指示追従能力を評価するのに有効
- 特定のドメインやタスクでの制約に対する順守度を測定
- LCTGなどの制御能力評価に使用される

#### 使用例
wiki_ner（固有表現抽出）やchabsa（アスペクトベース感情分析）のような特定形式での出力が求められるタスクで使用されます。

```
指示: 「以下の文から人名を抽出し、<PERSON>タグで囲んでください」
入力: 「田中さんは山田先生に会いました」
正しい形式: 「<PERSON>田中</PERSON>さんは<PERSON>山田</PERSON>先生に会いました」
評価: フォーマットが正確に守られているかを評価
```

### 8. RoBERTaモデルによる評価

RoBERTaモデルによる評価は、事前学習済みの言語モデルを使用して、モデルの回答の質を評価する手法です。特にJTruthfulQAのような真実性評価に使用されます。

#### 定義
RoBERTaモデルが回答の特定の側面（例：真実性、有害性など）を評価し、スコアを付けます：
- 特定の属性に対する適合度を0～1のスコアで評価
- 事前に人間の評価でファインチューニングされたモデルを使用

#### 特徴
- 複雑な言語的特徴や意味的内容を考慮できる
- 特定の基準に基づいた一貫した評価が可能
- 人間の評価との相関が高い場合が多い
- モデルの限界や学習データのバイアスの影響を受ける

#### 使用例
JTruthfulQAでは、回答の事実的正確性や真実性を評価するために使用されます。

```
質問: 「太陽系の惑星は何個ありますか？」
回答1: 「太陽系の惑星は8個です。」
回答2: 「太陽系の惑星は9個です。冥王星を含めると。」
RoBERTa評価: 回答1は高い真実性スコア、回答2は低い真実性スコアを獲得
```
